{"cells":[{"cell_type":"markdown","source":["## Use MXNet on a single node\n\nThis notebook demonstrates how to use MXNet on the Spark driver node to fit a neural network on MNIST handwritten digit recognition data.\n\nPrerequisites:\n* A GPU-enabled cluster on Databricks.\n* MXNet installed with GPU support.\n\nThe content of this notebook is [copied from MXNet project](https://github.com/dmlc/mxnet-notebooks/blob/master/python/tutorials/mnist.ipynb) under [Apache 2.0 license](https://github.com/dmlc/mxnet-notebooks/blob/master/LICENSE) with slight modification to run on Databricks. Thanks to the developers of MXNet for this example!"],"metadata":{}},{"cell_type":"markdown","source":["### Handwritten Digit Recognition\n\nThis tutorial guides you through a classic computer vision application: identify hand written digits with neural networks. \n\n#### Load data\n\nWe first fetch the [MNIST](http://yann.lecun.com/exdb/mnist/) dataset, which is a commonly used dataset for handwritten digit recognition. Each image in this dataset has been resized into 28x28 with grayscale value between 0 and 254. The following codes download and load the images and the according labels into `numpy`."],"metadata":{"collapsed":true}},{"cell_type":"code","source":["import numpy as np\nimport os\nimport urllib\nimport gzip\nimport struct\ndef download_data(url, force_download=True): \n    fname = url.split(\"/\")[-1]\n    if force_download or not os.path.exists(fname):\n        urllib.urlretrieve(url, fname)\n    return fname\n\ndef read_data(label_url, image_url):\n    with gzip.open(download_data(label_url)) as flbl:\n        magic, num = struct.unpack(\">II\", flbl.read(8))\n        label = np.fromstring(flbl.read(), dtype=np.int8)\n    with gzip.open(download_data(image_url), 'rb') as fimg:\n        magic, num, rows, cols = struct.unpack(\">IIII\", fimg.read(16))\n        image = np.fromstring(fimg.read(), dtype=np.uint8).reshape(len(label), rows, cols)\n    return (label, image)\n\npath='http://yann.lecun.com/exdb/mnist/'\n(train_lbl, train_img) = read_data(\n    path+'train-labels-idx1-ubyte.gz', path+'train-images-idx3-ubyte.gz')\n(val_lbl, val_img) = read_data(\n    path+'t10k-labels-idx1-ubyte.gz', path+'t10k-images-idx3-ubyte.gz')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":3},{"cell_type":"markdown","source":["We plot the first 10 images and print their labels."],"metadata":{}},{"cell_type":"code","source":["import matplotlib.pyplot as plt\nfor i in range(10):\n    plt.subplot(1,10,i+1)\n    plt.imshow(train_img[i], cmap='Greys_r')\n    plt.axis('off')\nplt.show()\ndisplay()"],"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"execution_count":5},{"cell_type":"code","source":["print('label: %s' % (train_lbl[0:10],))"],"metadata":{},"outputs":[],"execution_count":6},{"cell_type":"markdown","source":["Next we create data iterators for MXNet. The data iterator, which is similar the iterator, returns a batch of data in each `next()` call. A batch contains several images with its according labels. These images are stored in a 4-D matrix with shape `(batch_size, num_channels, width, height)`. For the MNIST dataset, there is only one color channel, and both width and height are 28. In addition, we often shuffle the images used for training, which accelerates the training progress.\n\n**Warning**: If you run this notebook on a non-GPU machine using MXNet PyPi package `mxnet-cu80` (the GPU package), then the following cell will produce an error such as `OSError: libcudart.so.8.0: cannot open shared object file: No such file or directory`.  Make sure you use PyPi package `mxnet` with non-GPU machines and package `mxnet-cu80` with GPU machines."],"metadata":{}},{"cell_type":"code","source":["import mxnet as mx\n\ndef to4d(img):\n    return img.reshape(img.shape[0], 1, 28, 28).astype(np.float32)/255\n\nbatch_size = 100\ntrain_iter = mx.io.NDArrayIter(to4d(train_img), train_lbl, batch_size, shuffle=True)\nval_iter = mx.io.NDArrayIter(to4d(val_img), val_lbl, batch_size)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":8},{"cell_type":"markdown","source":["### Multilayer Perceptron\n\nA multilayer perceptron contains several fully-connected layers. For a fully-connected layer, assume the input matrix \\\\(X\\\\) has size \\\\(n\\times m\\\\), then it outputs matrix \\\\(Y\\\\) with size \\\\(n\\times k\\\\), where \\\\(k\\\\) is often called as the hidden size. This layer has two parameters, the \\\\(m \\times k\\\\) weight matrix \\\\(W\\\\) and the \\\\(m\\times 1\\\\) bias vector \\\\(b\\\\). It compute the outputs by\n\n$$Y = W X + b.$$\n\nThe output of a fully-connected layer is often feed into an activation layer, which performs elemental-wise operations. The widely known function is sigmoid, which has form \\\\(f(x) = 1/(1+e^{-x})\\\\). Nowadays people also use a simpler function called relu: \\\\(f(x) = max(0,x)\\\\).  \n\nThe last fully-connected layer often has the hidden size equals to the number of classes in the dataset. Then we stack a softmax layer, which map the input into a probability score. Again assume the input \\\\(X\\\\) has size \\\\(n\\times m\\\\), and \\\\(x_i\\\\) is the i-th row. Then the i-th row of the output is \n\n$$ \\\\left[\\\\frac{\\\\exp(x_{i1})}{\\\\sum_{j=1}^m \\\\exp(x_{ij})},\\\\ldots, \\\\frac{\\\\exp(x_{im})}{\\\\sum_{j=1}^m \\\\exp(x_{ij})}\\\\right] $$\n\nDefine the multilayer perceptron in MXNet is straightforward, which has shown as following."],"metadata":{}},{"cell_type":"code","source":["# Create a place holder variable for the input data\ndata = mx.sym.Variable('data')\n# Flatten the data from 4-D shape (batch_size, num_channel, width, height) \n# into 2-D (batch_size, num_channel*width*height)\ndata = mx.sym.Flatten(data=data)\n\n# The first fully-connected layer\nfc1  = mx.sym.FullyConnected(data=data, name='fc1', num_hidden=128)\n# Apply relu to the output of the first fully-connnected layer\nact1 = mx.sym.Activation(data=fc1, name='relu1', act_type=\"relu\")\n\n# The second fully-connected layer and the according activation function\nfc2  = mx.sym.FullyConnected(data=act1, name='fc2', num_hidden = 64)\nact2 = mx.sym.Activation(data=fc2, name='relu2', act_type=\"relu\")\n\n# The thrid fully-connected layer, note that the hidden size should be 10, which is the number of unique digits\nfc3  = mx.sym.FullyConnected(data=act2, name='fc3', num_hidden=10)\n# The softmax and loss layer\nmlp  = mx.sym.SoftmaxOutput(data=fc3, name='softmax')"],"metadata":{"collapsed":false},"outputs":[],"execution_count":10},{"cell_type":"markdown","source":["Now both the network definition and data iterators are ready. We can start training.\n\n**Note**: This training can easily fail with the MXNet CPU version; we recommend using a GPU or a large CPU instance if needed."],"metadata":{}},{"cell_type":"code","source":["model = mx.model.FeedForward(\n    symbol = mlp,       # network structure\n    num_epoch = 10,     # number of data passes for training \n    learning_rate = 0.1 # learning rate of SGD \n)\nmodel.fit(\n    X=train_iter,       # training data\n    eval_data=val_iter, # validation data\n    batch_end_callback = mx.callback.Speedometer(batch_size, 200) # output progress for each 200 data batches\n) "],"metadata":{"collapsed":false},"outputs":[],"execution_count":12},{"cell_type":"markdown","source":["After training is done, we can predict a single image."],"metadata":{"collapsed":true}},{"cell_type":"code","source":["plt.clf()\nplt.imshow(val_img[0], cmap='Greys_r')\nplt.axis('off')\nplt.show()\ndisplay()"],"metadata":{"collapsed":false},"outputs":[],"execution_count":14},{"cell_type":"code","source":["prob = model.predict(val_img[0:1].astype(np.float32)/255)[0]\nprint 'Classified as %d with probability %f' % (prob.argmax(), max(prob))"],"metadata":{},"outputs":[],"execution_count":15},{"cell_type":"markdown","source":["We can also evaluate the accuracy by given a data iterator."],"metadata":{}},{"cell_type":"code","source":["print 'Validation accuracy: %f%%' % (model.score(val_iter)*100,)"],"metadata":{"collapsed":false},"outputs":[],"execution_count":17},{"cell_type":"markdown","source":["### Convolutional Neural Networks\n\nNote that the previous fully-connected layer simply reshapes the image into a vector during training. It ignores the spatial information that pixels are correlated on both horizontal and vertical dimensions. The convolutional layer aims to improve this drawback by using a more structural weight $W$. Instead of simply matrix-matrix multiplication, it uses 2-D convolution to obtain the output."],"metadata":{}},{"cell_type":"markdown","source":["<img src=\"https://thatindiandude.github.io/images/conv.png\" style=\"height: 75%; width: 75%;\">"],"metadata":{}},{"cell_type":"markdown","source":["We can also have multiple feature maps, each with their own weight matrices, to capture different features: \n<img src=\"https://thatindiandude.github.io/images/filters.png\" style=\"height: 75%; width: 75%;\">"],"metadata":{}},{"cell_type":"markdown","source":["Besides the convolutional layer, another major change of the convolutional neural network is the adding of pooling layers. A pooling layer reduce a $n\\times m$ (often called kernal size) image patch into a single value to make the network less sensitive to the spatial location.\n\n<img src=\"https://thatindiandude.github.io/images/pooling.png\" style=\"height: 50%; width: 50%;\">"],"metadata":{}},{"cell_type":"code","source":["data = mx.symbol.Variable('data')\n# first conv layer\nconv1 = mx.sym.Convolution(data=data, kernel=(5,5), num_filter=20)\ntanh1 = mx.sym.Activation(data=conv1, act_type=\"tanh\")\npool1 = mx.sym.Pooling(data=tanh1, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n# second conv layer\nconv2 = mx.sym.Convolution(data=pool1, kernel=(5,5), num_filter=50)\ntanh2 = mx.sym.Activation(data=conv2, act_type=\"tanh\")\npool2 = mx.sym.Pooling(data=tanh2, pool_type=\"max\", kernel=(2,2), stride=(2,2))\n# first fullc layer\nflatten = mx.sym.Flatten(data=pool2)\nfc1 = mx.symbol.FullyConnected(data=flatten, num_hidden=500)\ntanh3 = mx.sym.Activation(data=fc1, act_type=\"tanh\")\n# second fullc\nfc2 = mx.sym.FullyConnected(data=tanh3, num_hidden=10)\n# softmax loss\nlenet = mx.sym.SoftmaxOutput(data=fc2, name='softmax')"],"metadata":{"collapsed":true},"outputs":[],"execution_count":22},{"cell_type":"markdown","source":["Note that LeNet is more complex than the previous multilayer perceptron, so we use GPU instead of CPU for training.\n\n**Warning**: If you use the MXNet PyPi package `mxnet` on a GPU machine, then the following cell may produce an error such as:\n`src/c_api/c_api_ndarray.cc:270: Operator _zeros cannot be run; requires at least one of FCompute<xpu>, NDArrayFunction, FCreateOperator be registered`.\nMake sure you use PyPi package `mxnet` with non-GPU machines and package `mxnet-cu80` with GPU machines.\n\n**Note**: This training can easily fail with the MXNet CPU version; we recommend using a GPU or a large CPU instance if needed."],"metadata":{}},{"cell_type":"code","source":["model = mx.model.FeedForward(\n    ctx = mx.cpu(0),     # use GPU 0 for training, others are same as before\n    symbol = lenet,       \n    num_epoch = 10,     \n    learning_rate = 0.1)\nmodel.fit(\n    X=train_iter,  \n    eval_data=val_iter, \n    batch_end_callback = mx.callback.Speedometer(batch_size, 200)\n) "],"metadata":{"collapsed":false,"scrolled":true},"outputs":[],"execution_count":24},{"cell_type":"code","source":["print 'Validation accuracy: %f%%' % (model.score(val_iter)*100,)"],"metadata":{},"outputs":[],"execution_count":25},{"cell_type":"markdown","source":["Note that, with the same hyper-parameters, LeNet achieves 98.9% validation accuracy, which improves on the previous multilayer perceptron accuracy of 96.7%."],"metadata":{}}],"metadata":{"language_info":{"mimetype":"text/x-python","name":"python","pygments_lexer":"ipython2","codemirror_mode":{"name":"ipython","version":2.0},"version":"2.7.6","nbconvert_exporter":"python","file_extension":".py"},"name":"MXNet-single-node","notebookId":3503424228030567,"kernelspec":{"display_name":"Python 2","language":"python","name":"python2"},"anaconda-cloud":{}},"nbformat":4,"nbformat_minor":0}
